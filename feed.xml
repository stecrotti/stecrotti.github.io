<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://stecrotti.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://stecrotti.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-27T16:00:18+00:00</updated><id>https://stecrotti.github.io/feed.xml</id><title type="html">blank</title><subtitle>PhD in Physics </subtitle><entry><title type="html">PyTorch bits</title><link href="https://stecrotti.github.io/pybits/2024/pytorch-bits/" rel="alternate" type="text/html" title="PyTorch bits"/><published>2024-09-22T16:40:16+00:00</published><updated>2024-09-22T16:40:16+00:00</updated><id>https://stecrotti.github.io/pybits/2024/pytorch-bits</id><content type="html" xml:base="https://stecrotti.github.io/pybits/2024/pytorch-bits/"><![CDATA[<h2 id="tensors">Tensors</h2> <h3 id="broadcasting">Broadcasting</h3> <p>Tensor operations in PyTorch: the rules for what happens depending on the size of the input tensors to <code class="language-plaintext highlighter-rouge">torch.matmul</code> is found <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html">here</a>. See also the explanation at the <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">NumPy docs</a>.</p> <h3 id="selecting-multiple-rowscolumns">Selecting multiple rows/columns</h3> <p>Say you want to apply a given permutation <code class="language-plaintext highlighter-rouge">perm</code> to a 2D tensor <code class="language-plaintext highlighter-rouge">A</code>. In PyTorch you do</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span><span class="p">[</span><span class="n">perm</span><span class="p">][:,</span><span class="n">perm</span><span class="p">]</span>
</code></pre></div></div> <p>while in Julia you would simply do</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span><span class="x">[</span><span class="n">perm</span><span class="x">,</span><span class="n">perm</span><span class="x">]</span>
</code></pre></div></div> <h3 id="the-torch_scatter-package">The <code class="language-plaintext highlighter-rouge">torch_scatter</code> package</h3> <p>It’s a small auxiliary package (docs <a href="https://pytorch-scatter.readthedocs.io/en/latest/index.html">here</a>) to perform operations on tensors based on which group each element belongs to.</p> <p>Example: the way PyTorch Geometric combines GNNs together in a batch is to make a huge adjacency matrix and concatenating vector of features. Say you have a long vector of node features and another of the same length which keeps track of which indices refer to which graph in the batch: then passing the second vector to <code class="language-plaintext highlighter-rouge">scatter_*</code> will perform operation <code class="language-plaintext highlighter-rouge">*</code> to each group, returning a vector of length equal to the number of batches.</p> <h3 id="ones_like"><code class="language-plaintext highlighter-rouge">ones_like</code></h3> <p><code class="language-plaintext highlighter-rouge">torch.ones_like(A)</code>: used to construct a tensor full of 1s of the same size of the input argument.</p> <h2 id="misc">Misc</h2> <h3 id="view"><code class="language-plaintext highlighter-rouge">view</code></h3> <p>PyTorch’s <code class="language-plaintext highlighter-rouge">torch.Tensor.view(*size)</code> method accepts -1 as (at most) one of the provided dimensions to be inferred. Example: <code class="language-plaintext highlighter-rouge">x.view(-1)</code> returns a flattened version of <code class="language-plaintext highlighter-rouge">x</code>, whatever the original size.</p> <h3 id="chunk"><code class="language-plaintext highlighter-rouge">chunk</code></h3> <p><code class="language-plaintext highlighter-rouge">torch.chunk</code>==<code class="language-plaintext highlighter-rouge">torch.Tensor.chunk</code> splits a big array into chunks. Useful when you need to do linalg (matmul) operations on a set of matrices: first stack them together, then perform the operation which takes advantage of parallelism, fast underlying code etc. Finally retrieve each part using <code class="language-plaintext highlighter-rouge">chunk</code>.</p> <h3 id="refactoring">Refactoring</h3> <p>Apparently it’s a good habit to wrap pairs of (conv_layer, relu_layer) in a <code class="language-plaintext highlighter-rouge">torch.nn.Sequential</code> object, to keep things tidy and debug dimension inconsistencies.</p> <h3 id="item"><code class="language-plaintext highlighter-rouge">item</code></h3> <p>To retreive the content of a tensor with one element, use <code class="language-plaintext highlighter-rouge">Tensor.item()</code></p> <h3 id="storage-viewer">Storage viewer</h3> <p>The bare content of a <code class="language-plaintext highlighter-rouge">torch.Tensor</code> can be inspected via <code class="language-plaintext highlighter-rouge">tensor.untyped_storage</code>. For instance, when PyTorch expands a tensor for broadcasting, it will behave as the same row repeated three times, but that row is stored in memory only once.</p> <h3 id="stride"><code class="language-plaintext highlighter-rouge">stride</code></h3> <p>PyTorch has a <code class="language-plaintext highlighter-rouge">stride</code> method for tensors:</p> <blockquote> <p>stride(dim) -&gt; tuple or int <br/> Returns the stride of :attr:<code class="language-plaintext highlighter-rouge">self</code> tensor. <br/> Stride is the jump necessary to go from one element to the next one in the specified dimension :attr:<code class="language-plaintext highlighter-rouge">dim</code>. A tuple of all strides is returned when no argument is passed in.</p> </blockquote> <h3 id="use-of-ellipsis-">Use of ellipsis <code class="language-plaintext highlighter-rouge">...</code></h3> <p>When indexing a tensor, can use ellipsis <code class="language-plaintext highlighter-rouge">...</code> to indicate “all preceding/subsequent dimensions”</p> <h3 id="backpropagation">Backpropagation</h3> <p>The way PyTorch does backprop is by storing inputs and outputs at the time of the call to each intermediate function. The backward step is then computed by using those values.</p> <p>To define custom differentiable functions, write a class for the function, inheriting from <code class="language-plaintext highlighter-rouge">torch.autograd.Function</code>.</p> <h3 id="tensor-dimensions">Tensor dimensions</h3> <p>When dealing with tensors in pytorch, one could in principle decide arbitrarily how to organize the dimensions: channels, batch index, row, col. What is the preferred order? NCHW format: batch index, channel, row, col.</p> <h3 id="torch-dataset">Torch Dataset</h3> <p>A custom Dataset class must implement three functions: <code class="language-plaintext highlighter-rouge">__init__</code>, <code class="language-plaintext highlighter-rouge">__len__</code>, and <code class="language-plaintext highlighter-rouge">__getitem__</code>.</p> <h3 id="float-precision">Float precision</h3> <p>Use <code class="language-plaintext highlighter-rouge">torch.Tensor.half</code> to convert a tensor to half precision, much faster on a GPU.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Tensors]]></summary></entry><entry><title type="html">Python bits</title><link href="https://stecrotti.github.io/pybits/2024/python-bits/" rel="alternate" type="text/html" title="Python bits"/><published>2024-09-22T16:30:16+00:00</published><updated>2024-09-22T16:30:16+00:00</updated><id>https://stecrotti.github.io/pybits/2024/python-bits</id><content type="html" xml:base="https://stecrotti.github.io/pybits/2024/python-bits/"><![CDATA[<h2 id="performance">Performance</h2> <h3 id="vectorized-operations">Vectorized operations</h3> <p>Vectorized operations are quicker than loops in Python. That’s because loops are executed in Python, while vectorized operations done by NumPy call C routines under the hood, and C is overwhelmingly faster than Python</p> <h2 id="misc">Misc</h2> <h3 id="indexing">Indexing</h3> <p>Python’s way of indexing vectors is weird coming from Julia, but look at this: to split a list l before and after index i, you just do <code class="language-plaintext highlighter-rouge">before=l[:i]; after=l[i:]</code>.</p> <h3 id="dynamic-addition-of-attributes">Dynamic addition of attributes</h3> <p>Turns out in Python you can add attributes to an object at runtime, attributes that were not in the class definition! Things like <code class="language-plaintext highlighter-rouge">obj.new_attribute = 3</code>. It sounds pretty dangerous to me, but ok.</p> <h3 id="functors">Functors</h3> <p>Adding a <code class="language-plaintext highlighter-rouge">__call__</code> method to a class will enable instances to behave as functions. This is what is called “functor” behavior in Julia.</p> <h3 id="context-manager">Context manager</h3> <p>A context manager is a Python construct that calls <code class="language-plaintext highlighter-rouge">__enter__</code> when the object is created in a <code class="language-plaintext highlighter-rouge">with</code> clause, and <code class="language-plaintext highlighter-rouge">__exit__</code> at the end of the with clause. For instance, this is how Python handles the with <code class="language-plaintext highlighter-rouge">open(...) as f</code>: construct that you’ll often see for opening files without requiring an explicit <code class="language-plaintext highlighter-rouge">close(f)</code> at the end.</p> <h3 id="generators-and-iterators">Generators and iterators</h3> <p>Generators in Python are iterators, just like those in Julia, i.e. lazy iterators over iterables, except in Python once you can’t iterate more than once. Iterating a second time returns nothing</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; mygenerator = (x*x for x in range(3))
&gt;&gt;&gt; for i in mygenerator:
...    print(i)
0
1
4
&gt;&gt;&gt; for i in mygenerator:
...    print(i)
</code></pre></div></div> <p>Python iterators are stateful, meaning that an iterator is an object containing a state which is modified every time an iteration is performed. Instead, Julia’s <a href="https://docs.julialang.org/en/v1/base/collections/#Base.iterate"><code class="language-plaintext highlighter-rouge">iterate</code></a> accepts an iterator and a state, and returns a <code class="language-plaintext highlighter-rouge">(item , newstate)</code> tuple.</p> <p>In Python, iterators are managed using the <a href="https://www.geeksforgeeks.org/use-yield-keyword-instead-return-keyword-python/"><code class="language-plaintext highlighter-rouge">yield</code></a> keyword. Placed instead of a <code class="language-plaintext highlighter-rouge">return</code> statement, it transforms the return of the parent function into an iterator which, once iterated over, executes the code until the next <code class="language-plaintext highlighter-rouge">yield</code> statement. Automatically, enough information is stored in the iterator object such that it’s possible to resume execution at each iteration.</p> <p>Python comes with an <code class="language-plaintext highlighter-rouge">itertools</code> standard library. For example, <code class="language-plaintext highlighter-rouge">itertools.islice</code> is very similar to Julia’s <code class="language-plaintext highlighter-rouge">Iterators.take</code>.</p> <h3 id="file-explorer">File explorer</h3> <p><code class="language-plaintext highlighter-rouge">os.walk</code>: generates the file names in a directory tree by walking the tree either top-down or bottom-up. For each directory in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames, filenames).</p> <h3 id="the-pass-keyword">The <code class="language-plaintext highlighter-rouge">pass</code> keyword</h3> <p>It’s used as a “do nothing” operation in places where Python does not allow an empty space, e.g. class or function definitions, thus avoiding the raising of errors. It can be useful to define the skeleton of a program before actually filling in all the details, or in places where one knows there will be code in the future.</p> <h2 id="comparisons-with-julia">Comparisons with Julia</h2> <h3 id="closures">Closures</h3> <p><code class="language-plaintext highlighter-rouge">partial</code> is Python’s equivalent of a closure in Julia.</p> <h2 id="decorators">Decorators</h2> <p>Like Julia macros, decorators take a piece of code as input and return a modified version of it to be executed (metaprogramming). As an example, recall the <a href="https://www.fast.ai/posts/2019-08-06-delegation.html">discussion on delegation</a>.</p> <h3 id="import-of-decorator">Import of decorator</h3> <p>To import a decorator, you need to specify it without the “at” (@) at the beginning (not really sure why)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">numba</span> <span class="kn">import</span> <span class="n">njit</span>
<span class="nd">@njit</span>
<span class="n">do</span> <span class="n">stuff</span> <span class="bp">...</span>
</code></pre></div></div> <h2 id="numba">Numba</h2> <p><a href="https://numba.pydata.org/">Numba</a> is a library for compiling python code to machine code once, then running the fast version every time the function is called. Just like what happens all the time with Julia!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Performance Vectorized operations Vectorized operations are quicker than loops in Python. That’s because loops are executed in Python, while vectorized operations done by NumPy call C routines under the hood, and C is overwhelmingly faster than Python]]></summary></entry><entry><title type="html">Deep Learning bits</title><link href="https://stecrotti.github.io/pybits/2024/deep-learningbits/" rel="alternate" type="text/html" title="Deep Learning bits"/><published>2024-09-22T16:20:16+00:00</published><updated>2024-09-22T16:20:16+00:00</updated><id>https://stecrotti.github.io/pybits/2024/deep-learningbits</id><content type="html" xml:base="https://stecrotti.github.io/pybits/2024/deep-learningbits/"><![CDATA[<h2 id="gpu">GPU</h2> <h3 id="when-to-transfer-data">When to transfer data</h3> <p>It is usually a good idea to keep the data on the cpu and only load a batch to the gpu to compute the gradient of the loss at training time. This is because gpus typically have limited memory. However, gpus can also be used for their original application: image processing! If a lot of it is required, e.g. to do data augmentation, it can be advantageous to directly work with the dataset living on the gpu.</p> <h2 id="batch-normalization">Batch normalization</h2> <p>Pictorial explanation: with batchnorm each layer is less sensitive to the modifications in the parameters of earlier layers due to training. In this way each layer learns a bit more “independently”, effectively speeding up the process (see <a href="https://www.youtube.com/watch?v=nUUqwaxLnWs&amp;t=473s">video</a> by Andrew Ng).</p> <h3 id="alternatives-to-batchnorm">Alternatives to batchnorm</h3> <ul> <li>Group Normalization: small batch size can lead to poor accuracy in the mean and variance estimates -&gt; split not in batches but in channels, i.e. normalize with the mean and stdev for each channel, taken on all the training data.</li> <li>Instance normalization: while batchnorm applies the same normalization to all data in a batch, this one applied the normalization to each single datapoint. From what i understand the statistics is taken over internal dimension (rows, columns)</li> <li>Layer normalization: (you guessed it!) for a single data point, use avg and stdev from the whole layer. This eliminates dependence on batch size and apparently is more generalizable to recurrent nets</li> <li>RMSNorm: a variant of LayerNorm, scale invariant, more efficient to compute for large layers</li> </ul> <p>Question: batchnorm comprises, after standardization to mean 0 and stdev 1, a further re-scaling and offsetting. Isn’t this last step redundant as it could be absorbed into the next operation in the pipeline, which is a linear layer?</p> <p>Answer 1: the next operation could very well be a non-linear layer (e.g. ReLU).</p> <p>Answer 2: even if the next operation is linear, and the two could in principle be composed into a single linear layer, keeping the two separate is apprently good for stability during training, helps with vanishing/exploding gradients. Lastly, and this is the more convincing argument to me, in the finetuning phase of transfer learning, the scaling and offset parameters of batchnorm can be tuned to match the statistics of new data, while keeping the rest of the parameters frozen.</p> <h3 id="covariate-shift">Covariate shift</h3> <p>It’s a situation where the statistics of the input data change with respect to those of the training set. Basic solution: standardize the input. Observation: this exactly what batchnorm is doing, just not only at the input but in correspondence of each layer.</p> <h3 id="regularization-effect-of-batchnorm">Regularization effect of batchnorm</h3> <p>It somehow adds noise, which is known (e.g. dropout) to help with generalization. I’m not convinced about how there is more noise wrt regular batch training.</p> <h3 id="batchnorm-at-test-time">Batchnorm at test time</h3> <p>During training, you take statistics over the batch. At test time, one typically processes data one at the time. Solution: use means and variances computed during training and averaged across batches, reduced over iterations using an exponential moving average.</p> <h2 id="cnns">CNNs</h2> <h3 id="flexibility-of-convolutional-kernels">Flexibility of convolutional kernels</h3> <p>When instantiating a convolutional layer (e.g. with <code class="language-plaintext highlighter-rouge">torch.nn.Conv2d</code>), one doesn’t need to specify the size of the images! Indeed, the only parameters stored there are the entries of the kernel, whose size independent from the image’s.</p> <h3 id="fully-convolutional-networks">Fully Convolutional Networks</h3> <p>No fully connected layers, only convolutions.</p> <p><strong>Uses</strong>: As opposed to classification, for tasks where the output itself is high-dimensional, such as pixel-wise classification (Segmentation).</p> <p><strong>Advantages</strong>: Flexibility, as the sizes of the layers never depend on the input size. Less parameters, as convolutional kernels are typically much smaller than the input size.</p> <p><strong>Implementation</strong>: here, to get a scalar at the end, just average over the <code class="language-plaintext highlighter-rouge">height, width</code> dimensions.</p> <h3 id="1x1-layer">1x1 layer</h3> <p>A convolution layer with kernel of 1-by-1 pixels. It aggregates information from different channels, without mixing pixels.</p> <p>Uses:</p> <ul> <li>Change the number of channels without affecting spatial resolution</li> <li>Obvious: learn relationship between channels. Typically a non-linearity right after achieves this.</li> </ul> <h2 id="momentum">Momentum</h2> <p>Consider a damped system governed by Newton’s law</p> \[m\frac{d^2x}{dt^2}+\gamma\frac{dx}{dt}=-\nabla V\] <p>where the force is given by a viscous term proportional to the velocity $\frac{dx}{dt}$, and a space-dependent potential $V$.</p> <p>First, define the negative momentum $q=-m\frac{dx}{dt}$ and substitute to get a system of two differential equations</p> \[\begin{cases} \frac{dq}{dt}=-\frac\gamma m q+\nabla V\\ \frac{dx}{dt}=-\frac qm \end{cases}\] <p>Discretizing in the most naive way (I think this is called Euler’s method) gives</p> \[\begin{cases} q_{t+\Delta t}=\left(1-\frac\gamma m \Delta t\right)q_t+\Delta t\nabla V(x_t)\\ x_{t+\Delta t}=x_t-\frac{\Delta t}{m}q_{t+\Delta t} \end{cases}\] <p>Upon reparametrizing $z=\frac{q}{\Delta t}$, we get</p> \[\begin{cases} z_{t+\Delta t}=\left(\Delta t-\frac\gamma m\right)z_t+\nabla V(x_t)\\ x_{t+\Delta t}=x_t-\frac{\Delta t^2}{m}z_{t+\Delta t} \end{cases}\] <p>and finally, defining $\beta=\Delta t-\frac\gamma m$ and $\alpha=\frac{\Delta t^2}{m}$, we get</p> \[\begin{cases} z_{t+\Delta t}=\beta z_t+\nabla V(x_t)\\ x_{t+\Delta t}=x_t-\alpha z_{t+\Delta t} \end{cases}\] <p>which is the update rule for gradient descent with momentum.</p> <h2 id="resnet">ResNet</h2> <h3 id="residual-nets">Residual Nets</h3> <p>IDEA: learn the difference (residual) between input and output of a layer, instead of the transformation between them.</p> <p>Imagine starting from a trained 20-layer CNN. You are not satisfied with the results, want to go deeper. One thing you can do is to add another, say, 36 trainable layers and initialize them to do nothing (in a specific way), i.e. their transfer function is an identity. Now, fine-tune the parameters of those 36 layers. It looks like you either improve or at worst do nothing and performance stays the same as for the 20-layer net. This sounds convincing enough, but does it solve the vanishing gradient problem?</p> <p>Maybe the idea is that fitting zero is easier than fitting a complicated mapping $H(x)$. So, let the net learn $F(x)=H(x)-x$, then add $x$ back: evaluate $F(x)+x$.</p> <h2 id="scaling-of-parameters-at-initialization">Scaling of parameters at initialization</h2> <p>Consider inputs $x$ standardized with mean zero and std one. These are passing through a linear layer $w$ and then a ReLU layer. The output of the linear layer is \(a^i=\sum_{j=1}^{n_H}x^{(i)}_j w_j\) where $n_H$ is the size of the layer and $x^{(i)}_j$ is the $j$-th component of the $i$-th training point. After the ReLU layer, \(b^i = relu(a^i).\)</p> <p>Suppose we initialize $w$ with gaussian random numbers of mean zero and standard deviation $\sigma$. One sensible thing to say is to pick $\sigma$ such that the output of the layer has the same variance ($1$) as the input. The variance for the first layer is</p> \[\begin{align} \langle (a^i) ^2\rangle =&amp; \sum_{j,k}\langle x^{(i)}_j x^{(i)}_k\rangle\langle w_j w_k \rangle\\ =&amp;\sum_{j=1}^{n_H}\langle (x^{(i)}_j)^2 \rangle\langle (w_j)^2 \rangle\\ =&amp; \sum_{j=1}^{n_H}\sigma^2\\ =&amp;n_H\sigma^2. \end{align}\] <p>Now, $a^i$ is a gaussian with mean zero and variance $n_H\sigma^2$. What is the variance of $b^i=relu(a^i)$? We have</p> \[\begin{align} \langle b^i\rangle =&amp; \int_0^\infty da a \mathcal N(0,\sqrt{n_H}\sigma)\\ =&amp; \frac{\sqrt{n_H}\sigma}{2} \end{align}\] <p>and</p> \[\begin{align} \langle (b^i)^2\rangle =&amp; \int_0^\infty da a^2 \mathcal N(0,\sqrt{n_H}\sigma)\\ =&amp; \frac{n_h\sigma}{2}. \end{align}\] <p>So the variance is</p> \[Var(b^i)=\langle (b^i)^2\rangle-\langle b^i\rangle^2=\frac{n_H\sigma^2}{4.}\] <p>For it to be equal to one, we pick</p> \[\sigma=\sqrt{\frac{2}{n_H}}.\] <h2 id="old-style-machine-learning">Old-style Machine Learning</h2> <h3 id="decision-trees-and-random-forests">Decision trees and random forests</h3> <ul> <li>Decision trees are quick to train and easy to interpret.</li> <li>Random forests are trained on subset of rows and cols in the dataset. Idea: outputs from trees are uncorrelated -&gt; their errors cancel in the average.</li> <li>Train a RF to predict whether an item will be in the train or valid set, then look for the most relevant features, try removing them and see if accuracy stays ok. If so, the new validation set won’t contain out-of-domain data!</li> <li>Embeddings: important to encode relationship between predictors. Can be useful to first train NN on embeddings, then use embeddings instead of the actual data on a simpler model, e.g. RF.</li> </ul> <h2 id="useful-concepts">Useful concepts</h2> <h3 id="learning-rate-finder">Learning rate finder</h3> <p>As a preliminary step before training, one can do this to find a good learning rate: start with a very small one, do a step of gradient descent w.r.t a batch, then increase the learning rate (usually geometrically), and repeat while tracking the loss. Continue until the loss stops decreasing.</p> <h3 id="receptive-field">Receptive field</h3> <p>It’s the set of pixels in the original image that contribute to the value of a certain pixel at intermediate or output layers. Reminds me of the powers of the adjacency matrix of a graph which store information about nodes (pixels here) being reachable from others using paths of a certain maximum length.</p> <h3 id="1-cycle-training">1-cycle training</h3> <p>Learning rate schedule from small to large, to small again. Cool idea: in the epochs with large learning rate, the optimizer doesn’t get trapped in narrow local minima, instead is attracted by large basins, which are better for generalization. (cool idea, but is it true? High-dimensional landscapes can be deceptive)</p> <h3 id="cosine-annealing">Cosine annealing</h3> <p>It’s a type of learning rate schedule that has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being increased rapidly again. The resetting of the learning rate acts like a simulated restart of the learning process and the re-use of good weights as the starting point of the restart is referred to as a “warm restart” in contrast to a “cold restart” where a new set of small random numbers may be used as a starting point.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[GPU When to transfer data It is usually a good idea to keep the data on the cpu and only load a batch to the gpu to compute the gradient of the loss at training time. This is because gpus typically have limited memory. However, gpus can also be used for their original application: image processing! If a lot of it is required, e.g. to do data augmentation, it can be advantageous to directly work with the dataset living on the gpu.]]></summary></entry></feed>