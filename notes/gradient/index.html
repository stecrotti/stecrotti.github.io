<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Natural gradients | Stefano Crotti </title> <meta name="author" content="Stefano Crotti"> <meta name="description" content="PhD in Physics "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%A4&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stecrotti.github.io/notes/gradient/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Natural gradients",
            "description": "",
            "published": "June 24, 2025",
            "authors": [
              
              {
                "author": "Stefano Crotti",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Politecnico di Torino, Italy",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Stefano</span> Crotti </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">Portfolio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/notes/">Notes <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/pybits/index.html">PyBits </a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv.pdf">CV </a> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Natural gradients</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <p>Something about gradient descent has been bugging me. When you have a function \(L\) of some multivariate parameter \(\boldsymbol{\theta}\) and do gradient descent with learning rate \(\eta\) to minimize it \(\begin{align} \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \eta \nabla L(\boldsymbol{\theta}) \end{align}\) something about the units of measurement isn’t quite right. Suppose that \(L\) is a dimensionless quantity and the \(\boldsymbol{\theta}\)’s are all lengths, measured, say, in meters. The components of \(\nabla L=\{\frac{\partial L}{\partial \theta_1},\ldots, \frac{\partial L}{\partial \theta_d}\}\) have units of meters\(^{-1}\). But then the update \(\theta_i \leftarrow \theta_i -\eta\frac{\partial L}{\partial \theta_1}\) is adding up meters with meters\(^{-1}\). Horror! My elementary school teacher would say it’s like adding up potatoes with carrots.</p> <p>Notice that this is not just pointless mathematical meticolousness. Say that now you rescale the parameters so that they are measured in centimeters, do one iteration of gradient descent, then rescale back to meters. The overall result is <d-footnote>This ultimately comes from the fact that whenever units are rescaled, gradient components get rescaled in the same direction (they are co-variant) while vector components get rescaled in the opposite direction (they are contra-variant).</d-footnote> \(\begin{align} \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \frac{\eta}{10000} \nabla L(\boldsymbol{\theta}) \end{align}\) which to me looks convincing enough that the landscape in which gradient descent moves is quite sensitive to the way your parameters are measured. It could very well happen that some directions look “flat” when in reality they are varying, just on a much wider length scale. Will gradient descent arrive at the end of the flat region? Eventually, yes, but we clearly don’t want to wait too long.</p> <p>Anyway, maybe there is a solution: it could be that the learning parameter \(\eta\) is implicitly addressing the problem. If \(\eta\) has units meters\(^2\) everything is under control again. We could interpret this as: \(\eta\) is defining a <strong>scale</strong> for the parameters. If in the example above we had also rescaled \(\eta\) by the right amount, then updates \((1)\) and \((2)\) would have been equivalent. Cool, but what if now units are heterogeneous across the components of \(\boldsymbol{\theta}\)? After all there is only one \(\eta\) and it can’t have the right units for all of them.</p> <p>Let’s try to fix this by picking a different \(\eta_i\) in each direction to account for the scale of each parameter. Then the update would look something like \(\begin{align} \theta_i \leftarrow \theta_i - \eta_i \frac{\partial L}{\partial \theta_i} \end{align}\) This can’t work because the vector we are using for the update is not parallel to the gradient anymore. And we need to move in a direction parallel to the gradient in order to get to the minimum: we know that the gradient is the direction of steepest descent.</p> <p>It is, right?</p> <h2 id="steepest-descent">Steepest descent</h2> <p>Let’s review why one decides to move along the gradient in the first place. Starting from an initial point \(\boldsymbol{\theta}\), the goal is to make a small step \(\Delta\boldsymbol{\theta}\) in a direction that makes \(L\) decrease as much as possible. For simplicity let’s call \(\Delta\boldsymbol{\theta}=\varepsilon \boldsymbol{x}\) with \(\varepsilon\) some small constant. Let’s also fix the length of \(x\) since we are only interested in comparing directions: \(||\boldsymbol{x}||^2=1\). We are looking for \(\begin{align} \min_{||\boldsymbol{x}||^2=1} L(\boldsymbol{\theta}+\varepsilon \boldsymbol{x}) \end{align}\) Since the step is small, \(L\) is well approximated by a first-order expansion \(\begin{align} L(\boldsymbol{\theta}+\varepsilon \boldsymbol{x}) \approx L(\boldsymbol{\theta})+\varepsilon \sum_i x_i\frac{\partial L}{\partial \theta_i} \end{align}\) The optimization problem can be solved using a Lagrange multiplier \(\lambda\) that fixes the norm of \(\boldsymbol x\) \(\begin{align} 0 &amp;= \frac{\partial}{\partial x_i}\left[L(\boldsymbol{\theta})+\varepsilon \sum_j x_j\frac{\partial L}{\partial \theta_j} + \lambda\left(1-\sum_k x_k^2\right)\right]\\ &amp;= \varepsilon \frac{\partial L}{\partial \theta_i}-2\lambda x_i \end{align}\) which gives the old familiar result \(\begin{align} x = \frac{\nabla L(\boldsymbol{\theta})}{||\nabla L(\boldsymbol{\theta})||}. \end{align}\)</p> <p>Notice though, that I made an arbitrary choice for what the “length”, or the “norm” of \(\boldsymbol x\) was. In general, if parameters live in some space \(\mathcal M\), such space can be equipped with an operation that measures the length of vectors tangent to \(\mathcal M\). Mathematicians call it a <strong>metric</strong>. At a point \(\boldsymbol \theta\), the action of a metric \(G\) on a pair of vectors \(\boldsymbol x, \boldsymbol y\) tangent to \(\mathcal M\) at \(\boldsymbol \theta\) is given by \(\begin{align} G_\theta(\boldsymbol x, \boldsymbol y)=\sum_{ij}g_{ij}(\boldsymbol \theta)x_i y_j \end{align}\) where \(g_{ij}, x_i, y_j\) are the components of \(G, \boldsymbol x, \boldsymbol y\) with respect to some basis. The length of a vector \(\boldsymbol x\) is \(||\boldsymbol x||_G=G_\theta(\boldsymbol x, \boldsymbol x)=\sum_{ij}g_{ij}(\boldsymbol\theta)x_i x_j\).</p> <p>Picking \(g(\boldsymbol \theta)_{ij}=\delta_{ij}\forall\boldsymbol \theta\) gives back the norm in Euclidean space.</p> <h2 id="natural-gradient">Natural gradient</h2> <p>If we re-do the optimization problem \((4)\) using an arbitrary metric \(G\) to measure vectors, we get \(\begin{align} 0 &amp;= \frac{\partial}{\partial x_i}\left[L(\boldsymbol{\theta})+\varepsilon \sum_j x_j\frac{\partial L}{\partial \theta_j} + \lambda\left(1-\sum_{kl} g_{kl}(\boldsymbol\theta)x_kx_l\right)\right]\\ &amp;= \varepsilon \frac{\partial L}{\partial \theta_i}-2\lambda \sum_k x_k g_{ki}(\boldsymbol\theta) \end{align}\) which gives \(\begin{align} \boldsymbol x = \frac{G(\boldsymbol\theta)^{-1}\nabla L(\boldsymbol{\theta})}{||\nabla L(\boldsymbol{\theta})||_G} \end{align}\) which is known as the “natural gradient”.</p> <p>So there it is: <strong>the direction of steepest descent in a space with a non-trivial metric is given by the natural gradient</strong>.</p> <p>It must be pointed out that the computational cost with respect to regular gradient descent is quite larger: it scales as the cube of the number of parameters. This becomes challenging when your problem has millions of parameters.</p> <h2 id="which-metric">Which metric?</h2> <p>This is a neat piece of theory, but what does one do in practice? Is there a good criterion to pick a metric \(G\)?</p> <p>One choice can be a diagonal metric: for each parameter \(i\), manually provide some characteristic quantity giving the scale of that parameter. Raise it to the power \(-2\) and set it as the diagonal element \(g_{ii}\). Off-diagonal terms are all zero. This recovers the update proposed heuristically in \((3)\). Since inverting a diagonal matrix is trivial, using such a metric gives essentially no computational overhead.</p> <p>There is a case where a choice for the metric comes up quite naturally: variational (Bayesian) inference. Given an intractable distribution \(p(\boldsymbol x)\), propose a family of distributions \(q_{\boldsymbol \theta}(\boldsymbol x)\) parametrized by \(\theta\). Then look for the choice of parameters that minimizes the Kullback-Leibler divergence, or relative entropy, between \(q_{\boldsymbol \theta}\) and \(p\): \(\begin{align} D(q_{\boldsymbol \theta}||p)=\sum_{\boldsymbol x}q_{\boldsymbol \theta}(\boldsymbol x)\log\frac{q_{\boldsymbol \theta}(\boldsymbol x)}{p(\boldsymbol x)} \end{align}\) So, the scenario is: we have a space of parameters and want to define a function measuring the distance between two nearby points in this space. Since each choice of \(\boldsymbol \theta\) corresponds to a probability distribution \(q_{\boldsymbol \theta}\), we could define the distance between two points in parameter space as some distance between the distributions they parametrize. The most natural choice <d-footnote>Natural is of course a matter of taste, but if you like physics and information theory, you'll probably agree with me here.</d-footnote> is again the KL divergence. The only problem is that, as the name suggests, KL is not a well-defined distance function. For example it’s not symmetric \(D(p||q)\neq D(q||p)\). However, if we limit ourselves to measuring the distance between points that are close to one another, we get, to second order \(\begin{align} D(q_{\boldsymbol \theta}||q_{\boldsymbol \theta+\Delta\boldsymbol \theta})\approx \sum_{ij}g_{ij}(\boldsymbol \theta)\Delta\theta_i\Delta\theta_j \end{align}\) where \(\begin{align} g_{ij}(\boldsymbol \theta)= \sum_{\boldsymbol x}q_{\boldsymbol \theta}(\boldsymbol x)\left[ \frac{\partial \log q_{\boldsymbol \theta}(\boldsymbol x)}{\partial \theta_i} \frac{\partial \log q_{\boldsymbol \theta}(\boldsymbol x)}{\partial \theta_j}\right] \end{align}\) is quite a famous object: the <strong>Fisher informatrion metric</strong>.</p> <h2 id="summary">Summary</h2> <p>Minimizing a function via gradient descent can be done using the natural gradient induced by a certain metric. This is expected to give better results than regular gradient descent, albeit at the possibly considerable cost of computing and inverting \(g_{ij}(\boldsymbol \theta)\).</p> <p>Personally I see this as a nice piece of theory, I’m not expecting to actually make use of natural gradients anytime soon. That said, learning about it made me meditate first on units of measurement and then on how one can make the Fisher information metric pop up naturally.</p> <p>The reference for this is mostly <d-cite key="amari1998natural"></d-cite>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/gradient.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'stecrotti/stecrotti.github.io',
        'data-repo-id': 'MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==',
        'data-category': 'Comments',
        'data-category-id': 'DIC_kwDOA5PmLc4CTBt6',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Stefano Crotti. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 24, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>