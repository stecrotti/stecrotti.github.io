<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PyTorch bits | Stefano Crotti </title> <meta name="author" content="Stefano Crotti"> <meta name="description" content="PhD in Physics "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%A4&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://stecrotti.github.io/pybits/2024/pytorch-bits/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "PyTorch bits",
            "description": "",
            "published": "September 22, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Stefano</span> Crotti </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">Notes </a> </li> <li class="nav-item active"> <a class="nav-link" href="/pybits/index.html">PyBits </a> </li> <li class="nav-item "> <a class="nav-link" href="/Links/">Links </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>PyTorch bits</h1> <p></p> </d-title> <d-article> <h2 id="tensors">Tensors</h2> <h3 id="broadcasting">Broadcasting</h3> <p>Tensor operations in PyTorch: the rules for what happens depending on the size of the input tensors to <code class="language-plaintext highlighter-rouge">torch.matmul</code> is found <a href="https://pytorch.org/docs/stable/generated/torch.matmul.html" rel="external nofollow noopener" target="_blank">here</a>. See also the explanation at the <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html" rel="external nofollow noopener" target="_blank">NumPy docs</a>.</p> <h3 id="selecting-multiple-rowscolumns">Selecting multiple rows/columns</h3> <p>Say you want to apply a given permutation <code class="language-plaintext highlighter-rouge">perm</code> to a 2D tensor <code class="language-plaintext highlighter-rouge">A</code>. In PyTorch you do</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span><span class="p">[</span><span class="n">perm</span><span class="p">][:,</span><span class="n">perm</span><span class="p">]</span>
</code></pre></div></div> <p>while in Julia you would simply do</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span><span class="x">[</span><span class="n">perm</span><span class="x">,</span><span class="n">perm</span><span class="x">]</span>
</code></pre></div></div> <h3 id="the-torch_scatter-package">The <code class="language-plaintext highlighter-rouge">torch_scatter</code> package</h3> <p>It’s a small auxiliary package (docs <a href="https://pytorch-scatter.readthedocs.io/en/latest/index.html" rel="external nofollow noopener" target="_blank">here</a>) to perform operations on tensors based on which group each element belongs to.</p> <p>Example: the way PyTorch Geometric combines GNNs together in a batch is to make a huge adjacency matrix and concatenating vector of features. Say you have a long vector of node features and another of the same length which keeps track of which indices refer to which graph in the batch: then passing the second vector to <code class="language-plaintext highlighter-rouge">scatter_*</code> will perform operation <code class="language-plaintext highlighter-rouge">*</code> to each group, returning a vector of length equal to the number of batches.</p> <h3 id="ones_like"><code class="language-plaintext highlighter-rouge">ones_like</code></h3> <p><code class="language-plaintext highlighter-rouge">torch.ones_like(A)</code>: used to construct a tensor full of 1s of the same size of the input argument.</p> <h2 id="misc">Misc</h2> <h3 id="view"><code class="language-plaintext highlighter-rouge">view</code></h3> <p>PyTorch’s <code class="language-plaintext highlighter-rouge">torch.Tensor.view(*size)</code> method accepts -1 as (at most) one of the provided dimensions to be inferred. Example: <code class="language-plaintext highlighter-rouge">x.view(-1)</code> returns a flattened version of <code class="language-plaintext highlighter-rouge">x</code>, whatever the original size.</p> <h3 id="chunk"><code class="language-plaintext highlighter-rouge">chunk</code></h3> <p><code class="language-plaintext highlighter-rouge">torch.chunk</code>==<code class="language-plaintext highlighter-rouge">torch.Tensor.chunk</code> splits a big array into chunks. Useful when you need to do linalg (matmul) operations on a set of matrices: first stack them together, then perform the operation which takes advantage of parallelism, fast underlying code etc. Finally retrieve each part using <code class="language-plaintext highlighter-rouge">chunk</code>.</p> <h3 id="refactoring">Refactoring</h3> <p>Apparently it’s a good habit to wrap pairs of (conv_layer, relu_layer) in a <code class="language-plaintext highlighter-rouge">torch.nn.Sequential</code> object, to keep things tidy and debug dimension inconsistencies.</p> <h3 id="item"><code class="language-plaintext highlighter-rouge">item</code></h3> <p>To retreive the content of a tensor with one element, use <code class="language-plaintext highlighter-rouge">Tensor.item()</code></p> <h3 id="storage-viewer">Storage viewer</h3> <p>The bare content of a <code class="language-plaintext highlighter-rouge">torch.Tensor</code> can be inspected via <code class="language-plaintext highlighter-rouge">tensor.untyped_storage</code>. For instance, when PyTorch expands a tensor for broadcasting, it will behave as the same row repeated three times, but that row is stored in memory only once.</p> <h3 id="stride"><code class="language-plaintext highlighter-rouge">stride</code></h3> <p>PyTorch has a <code class="language-plaintext highlighter-rouge">stride</code> method for tensors:</p> <blockquote> <p>stride(dim) -&gt; tuple or int <br> Returns the stride of :attr:<code class="language-plaintext highlighter-rouge">self</code> tensor. <br> Stride is the jump necessary to go from one element to the next one in the specified dimension :attr:<code class="language-plaintext highlighter-rouge">dim</code>. A tuple of all strides is returned when no argument is passed in.</p> </blockquote> <h3 id="use-of-ellipsis-">Use of ellipsis <code class="language-plaintext highlighter-rouge">...</code> </h3> <p>When indexing a tensor, can use ellipsis <code class="language-plaintext highlighter-rouge">...</code> to indicate “all preceding/subsequent dimensions”</p> <h3 id="backpropagation">Backpropagation</h3> <p>The way PyTorch does backprop is by storing inputs and outputs at the time of the call to each intermediate function. The backward step is then computed by using those values.</p> <p>To define custom differentiable functions, write a class for the function, inheriting from <code class="language-plaintext highlighter-rouge">torch.autograd.Function</code>.</p> <h3 id="tensor-dimensions">Tensor dimensions</h3> <p>When dealing with tensors in pytorch, one could in principle decide arbitrarily how to organize the dimensions: channels, batch index, row, col. What is the preferred order? NCHW format: batch index, channel, row, col.</p> <h3 id="torch-dataset">Torch Dataset</h3> <p>A custom Dataset class must implement three functions: <code class="language-plaintext highlighter-rouge">__init__</code>, <code class="language-plaintext highlighter-rouge">__len__</code>, and <code class="language-plaintext highlighter-rouge">__getitem__</code>.</p> <h3 id="float-precision">Float precision</h3> <p>Use <code class="language-plaintext highlighter-rouge">torch.Tensor.half</code> to convert a tensor to half precision, much faster on a GPU.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Stefano Crotti. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>